# jemdoc: menu{MENU}{index.html}
= Daniel Russo

~~~
{}{img_left}{pics/photo3.jpg}{}{}{250}{}
Philip H. Geier Jr. Associate Professor at Columbia Business School  \n
[djr2174@gsb.columbia.edu] \n
[https://scholar.google.com/citations?user=MM5QyU4AAAAJ&hl=en Google Scholar Profile] \n
[http://www.linkedin.com/pub/daniel-russo/45/743/970 Linkedin profile] \n


~~~

== About Me 

I joined the [https://www8.gsb.columbia.edu/faculty-research/divisions/decision-risk-operations Decision, Risk, and Operations division] of the Columbia Business School in Summer 2017. I teach a core MBA course on statistics and a PhD course on dynamic optimization. My research lies at the intersection of statistical machine learning and online decision making, mostly falling under the broad umbrella of reinforcement learning.
Outside academia, I work with Spotify to apply reinforcement learning and large language models to audio recommendations.  

My research has been recognized by the Frederick W. Lanchester Prize, a Junior Faculty Interest Group Best Paper Award, and first place in the George Nicholson Student Paper Competition. I currently serve as an associate editor at Management Science, Operations Research, and Stochastic Systems.


Prior to joining Columbia, I spent one great year  as an assistant professor at Northwestern's Kellogg School of Management
and one year at Microsoft Research in New England as Postdoctoral Researcher. I received my PhD  from Stanford University in 2015, where I was advised by [http://engineering.stanford.edu/profile/bvr Benjamin Van Roy].
In 2011 I received my BS in Mathematics and Economics from the University of Michigan. 



#== Research area  

#I work on a subfield of machine learning called reinforcement #learning (RL). I mean this rather broadly, as grappling with key #issues that are not a focus of standard (supervised) machine learning:

#1. The goal is to make effective decisions. Better predictions are #useful insofar as they advance this goal.    
	
#- In these problems, one must take seriously the specification of the decision-objective (e.g. the definition of the "reward" in RL lingo) and the subtle way in which estimation errors influence the quality of resulting decisions. I've been interested in the interplay between a problem's time horizon and robusness to mis-estimation (see e.g. [https://arxiv.org/abs/2102.10025 *here*], or [https://arxiv.org/abs/2007.11684 *here*]) and in measures of the marginal value of information that are suitable for decision-making (see [https://pubsonline.informs.org/doi/abs/10.1287/opre.2017.1663 *here*] or [https://arxiv.org/abs/1803.02855 *here*]). 

	
#2. Decisions today influence the data available in the future. 

#- Learning may require purposeful experimentation. Think of TikTok, where data is only collected on recomended videos, and data is fed back into the system to drive future reccomendations. I've worked extensively on methods for efficient experimentation (see [https://pubsonline.informs.org/doi/abs/10.1287/opre.2017.1663 *here*], [docs/Learning_to_Optimize.pdf *here*] or [docs/best_arm_identification_body.pdf *here*]
#) and have also worked a bit on issues of statistical bias (this is #implicitly delt with in many papers and explicit #[docs/Information_usage.pdf *here*]).



#3. Decisions have delayed consequences, introducing intertemporal tradeoffs and challenges with mesurement and attribution. 

#- RL is the machine learning paradigm that deals with making a sequence of decisions to attain a goal. Key challenges include accurate measurement ( long-term outcomes often have very high variance and are observed after long delay) and attribution (a long sequence of actions jointly cause a good outcome, making it unclear which behaviors to reinforce).  My applied work at Spotify mainly focuses on these challenges, and the intuition from having worked on foundational RL approaches has been quite helpful (e.g. [docs/TD_finite_time-merged.pdf *TD*] is about sidestepping measurement issues and [https://arxiv.org/abs/1906.01786   *local policy improvement*] is about coherently deciding which behaviors to reinforce).   


#== Selected Papers  


#[https://arxiv.org/abs/2202.09036 *Adaptivity and Confounding in Multi-Armed Bandit Experiments*], Chao Qin and Daniel Russo, working paper. 


#[https://arxiv.org/abs/1906.01786 *Global Optimality Guarantees For Policy Gradient Methods*], Jalaj Bhandari and Daniel Russo, working paper.


#[docs/best_arm_identification_body.pdf *Simple Bayesian Algorithms for Best Arm Identification*],
#Daniel Russo, Operations Research, 2020.

#[docs/Information_usage.pdf *Controling Bias in Adaptive Data Analysis Using Information Theory*],
# Daniel Russo and James Zou,IEEE Transaction on Information Theory, 2020

#[https://pubsonline.informs.org/doi/abs/10.1287/opre.2017.1663 *Learning to Optimize Via Information Directed Sampling*], 
#Daniel Russo and Benjamin Van Roy, Operations Research, 2018


#[docs/Learning_to_Optimize.pdf *Learning to Optimize Via Posterior Sampling*], Daniel Russo and Benjamin Van Roy,
#Mathematics of Operations Research. 2014.


#[docs/TS_Tutorial.pdf *A Tutorial on Thompson Sampling*],
#Daniel Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband, and Zheng Wen, Foundations and Trends in Machine Learning, 2018.

#These papers offer an introduction to my research interests and taste. 

#== Contact Information

#[http://campus-map.stanford.edu/?id=&lat=37.434869229867694&lng=-122.17429865&zoom=15&srch=Jen-Hsun Huang Engineering Center Huang 141K] \n
#Stanford, CA 94305 \n
#[djrusso@stanford.edu] \n
#[http://www.linkedin.com/pub/daniel-russo/45/743/970 Linkedin profile] \n
#[http://scholar.google.com/citations?hl=en&user=QA4o6eYAAAAJ&view_op=list_works&gmla=AJsN-F4DfekwaRlNu3zwrbj6hlUEL8K75ehbWbdyGF1Ap3-i0MAOJCMcaN9rvq0S72nCqmOC0yNB4pa32Tc9FfMyYZVes9Sy3fLiAQ3RWs80bBM9C525gFA2-lyDyg5Mv3-_tzElTz7c Google Scholar]

