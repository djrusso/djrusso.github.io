<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Daniel Russo</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Site Map</div>
<div class="menu-item"><a href="index.html" class="current">Home</a></div>
<div class="menu-item"><a href="research.html">Research</a></div>
<div class="menu-item"><a href="talk_videos.html">Talk&nbsp;Videos</a></div>
<div class="menu-item"><a href="teaching.html">Teaching</a></div>
<div class="menu-item"><a href="docs/CV.pdf">Resume</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Daniel Russo</h1>
</div>
<table class="imgtable"><tr><td>
<img src="pics/photo3.jpg" alt="" height="250px" />&nbsp;</td>
<td align="left"><p>Associate Professor at Columbia Business School  <br />
<a href="mailto:djr2174@gsb.columbia.edu">djr2174@gsb.columbia.edu</a> <br />
<a href="https://scholar.google.com/citations?user=MM5QyU4AAAAJ&amp;hl=en">Google Scholar Profile</a> <br />
<a href="http://www.linkedin.com/pub/daniel-russo/45/743/970">Linkedin profile</a> <br /></p>
</td></tr></table>
<h2>About Me </h2>
<p>I joined the <a href="https://www8.gsb.columbia.edu/faculty-research/divisions/decision-risk-operations">Decision, Risk, and Operations division</a> of the Columbia Business School in Summer 2017. I teach a core MBA course on statistics and a PhD course on dyanamic optimization. My research lies at the intersection of statistical machine learning and online decision making, mostly falling under the broad umbrella of reinforcement learning. 
Outside academia, I work with Spotify to apply reinforcement learning style models to audio recommendations.  </p>
<p>Prior to joining Columbia, I spent one great year  as an assistant professor in the MEDS department at Northwestern's Kellogg School of Management 
and one year at Microsoft Research in New England as Postdoctoral Researcher. I recieved my PhD  from Stanford University in 2015, where I was advised by <a href="http://engineering.stanford.edu/profile/bvr">Benjamin Van Roy</a>. 
In 2011 I recieved my BS in Mathematics and Economics from the University of Michigan. </p>
<p>I currently serve as an associate editor at Management Science and Stochastic Systems. </p>
<h2>Research area  </h2>
<p>I work on a subfield of machine learning called reinforcement learning (RL). I mean this rather broadly, as grappling with key issues that are not a focus of standard (supervised) machine learning:</p>
<p>1. The goal is to make effective decisions. Better predictions are useful insofar as they advance this goal.    </p>
<ul>
<li><p>In these problems, one must take seriously the specification of the decision-objective (e.g. the definition of the &ldquo;reward&rdquo; in RL lingo) and the subtle way in which estimation errors influence the quality of resulting decisions. I've been interested in the interplay between a problem's time horizon and robusness to mis-estimation (see e.g. <a href="https://arxiv.org/abs/2102.10025"><b>here</b></a>, or <a href="https://arxiv.org/abs/2007.11684"><b>here</b></a>) and in measures of the marginal value of information that are suitable for decision-making (see <a href="https://pubsonline.informs.org/doi/abs/10.1287/opre.2017.1663"><b>here</b></a> or <a href="https://arxiv.org/abs/1803.02855"><b>here</b></a>). </p>
</li>
</ul>
<p>2. Decisions today influence the data available in the future. </p>
<ul>
<li><p>Learning may require purposeful experimentation. Think of TikTok, where data is only collected on recomended videos, and data is fed back into the system to drive future reccomendations. I've worked extensively on methods for efficient experimentation (see <a href="https://pubsonline.informs.org/doi/abs/10.1287/opre.2017.1663"><b>here</b></a>, <a href="docs/Learning_to_Optimize.pdf"><b>here</b></a> or <a href="docs/best_arm_identification_body.pdf"><b>here</b></a>
) and have also worked a bit on issues of statistical bias (this is implicitly delt with in many papers and explicit <a href="docs/Information_usage.pdf"><b>here</b></a>).</p>
</li>
</ul>
<p>3. Decisions have delayed consequences, introducing intertemporal tradeoffs and challenges with mesurement and attribution. </p>
<ul>
<li><p>RL is the machine learning paradigm that deals with making a sequence of decisions to attain a goal. Key challenges include accurate measurement ( long-term outcomes often have very high variance and are observed after long delay) and attribution (a long sequence of actions jointly cause a good outcome, making it unclear which behaviors to reinforce).  My applied work at Spotify mainly focuses on these challenges, and the intuition from having worked on foundational RL approaches has been quite helpful (e.g. <a href="docs/TD_finite_time-merged.pdf"><b>TD</b></a> is about sidestepping measurement issues and <a href="https://arxiv.org/abs/1906.01786"><b>local policy improvement</b></a> is about coherently deciding which behaviors to reinforce).   </p>
</li>
</ul>
<div id="footer">
<div id="footer-text">
Page generated 2021-11-28 08:31:36 Cuba Standard Time, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
